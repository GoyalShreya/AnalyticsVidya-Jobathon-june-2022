{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "46ef6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b06bd7",
   "metadata": {},
   "source": [
    "# DATA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a3803524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_wn75k28.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a089e69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>campaign_var_1</th>\n",
       "      <th>campaign_var_2</th>\n",
       "      <th>products_purchased</th>\n",
       "      <th>user_activity_var_1</th>\n",
       "      <th>user_activity_var_2</th>\n",
       "      <th>user_activity_var_3</th>\n",
       "      <th>user_activity_var_4</th>\n",
       "      <th>user_activity_var_5</th>\n",
       "      <th>user_activity_var_6</th>\n",
       "      <th>user_activity_var_7</th>\n",
       "      <th>user_activity_var_8</th>\n",
       "      <th>user_activity_var_9</th>\n",
       "      <th>user_activity_var_10</th>\n",
       "      <th>user_activity_var_11</th>\n",
       "      <th>user_activity_var_12</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>18250.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "      <td>39161.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19581.000000</td>\n",
       "      <td>6.523812</td>\n",
       "      <td>6.452746</td>\n",
       "      <td>2.154137</td>\n",
       "      <td>0.400092</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.102832</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.151503</td>\n",
       "      <td>0.499834</td>\n",
       "      <td>0.286612</td>\n",
       "      <td>0.174434</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.218942</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.051020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11304.951283</td>\n",
       "      <td>3.472944</td>\n",
       "      <td>2.614296</td>\n",
       "      <td>0.779815</td>\n",
       "      <td>0.509194</td>\n",
       "      <td>0.081676</td>\n",
       "      <td>0.303743</td>\n",
       "      <td>0.106463</td>\n",
       "      <td>0.359681</td>\n",
       "      <td>0.558166</td>\n",
       "      <td>0.455784</td>\n",
       "      <td>0.379689</td>\n",
       "      <td>0.106346</td>\n",
       "      <td>0.019568</td>\n",
       "      <td>0.431544</td>\n",
       "      <td>0.023696</td>\n",
       "      <td>0.220042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9791.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>19581.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29371.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39161.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  campaign_var_1  campaign_var_2  products_purchased  \\\n",
       "count  39161.000000    39161.000000    39161.000000        18250.000000   \n",
       "mean   19581.000000        6.523812        6.452746            2.154137   \n",
       "std    11304.951283        3.472944        2.614296            0.779815   \n",
       "min        1.000000        1.000000        1.000000            1.000000   \n",
       "25%     9791.000000        4.000000        5.000000            2.000000   \n",
       "50%    19581.000000        6.000000        6.000000            2.000000   \n",
       "75%    29371.000000        9.000000        8.000000            3.000000   \n",
       "max    39161.000000       16.000000       15.000000            4.000000   \n",
       "\n",
       "       user_activity_var_1  user_activity_var_2  user_activity_var_3  \\\n",
       "count         39161.000000         39161.000000         39161.000000   \n",
       "mean              0.400092             0.006716             0.102832   \n",
       "std               0.509194             0.081676             0.303743   \n",
       "min               0.000000             0.000000             0.000000   \n",
       "25%               0.000000             0.000000             0.000000   \n",
       "50%               0.000000             0.000000             0.000000   \n",
       "75%               1.000000             0.000000             0.000000   \n",
       "max               3.000000             1.000000             1.000000   \n",
       "\n",
       "       user_activity_var_4  user_activity_var_5  user_activity_var_6  \\\n",
       "count         39161.000000         39161.000000         39161.000000   \n",
       "mean              0.011465             0.151503             0.499834   \n",
       "std               0.106463             0.359681             0.558166   \n",
       "min               0.000000             0.000000             0.000000   \n",
       "25%               0.000000             0.000000             0.000000   \n",
       "50%               0.000000             0.000000             0.000000   \n",
       "75%               0.000000             0.000000             1.000000   \n",
       "max               1.000000             2.000000             3.000000   \n",
       "\n",
       "       user_activity_var_7  user_activity_var_8  user_activity_var_9  \\\n",
       "count         39161.000000         39161.000000         39161.000000   \n",
       "mean              0.286612             0.174434             0.011440   \n",
       "std               0.455784             0.379689             0.106346   \n",
       "min               0.000000             0.000000             0.000000   \n",
       "25%               0.000000             0.000000             0.000000   \n",
       "50%               0.000000             0.000000             0.000000   \n",
       "75%               1.000000             0.000000             0.000000   \n",
       "max               2.000000             2.000000             1.000000   \n",
       "\n",
       "       user_activity_var_10  user_activity_var_11  user_activity_var_12  \\\n",
       "count          39161.000000          39161.000000          39161.000000   \n",
       "mean               0.000383              0.218942              0.000562   \n",
       "std                0.019568              0.431544              0.023696   \n",
       "min                0.000000              0.000000              0.000000   \n",
       "25%                0.000000              0.000000              0.000000   \n",
       "50%                0.000000              0.000000              0.000000   \n",
       "75%                0.000000              0.000000              0.000000   \n",
       "max                1.000000              4.000000              1.000000   \n",
       "\n",
       "                buy  \n",
       "count  39161.000000  \n",
       "mean       0.051020  \n",
       "std        0.220042  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab701805",
   "metadata": {},
   "source": [
    "# Data Splitting using Startified split over Buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4ede400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2, stratify=df['buy'], shuffle=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "504a854b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31328, 7833)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9fbe6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train['buy']\n",
    "train_x = train.drop('buy', axis=1)\n",
    "test_y = test['buy']\n",
    "test_x = test.drop('buy', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59429c",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "79fee9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data[['created_at', 'signup_date']] = data[['created_at', 'signup_date']].apply(pd.to_datetime)\n",
    "    max_date = data['created_at'].max()\n",
    "    data['created_days'] = data['created_at'].apply(lambda x: (max_date-x).days)\n",
    "    data['is_signed'] = data['signup_date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "    data['signed_days'] = data.apply(lambda x: (x['created_at']-x['signup_date']).days, axis=1).fillna(-1)\n",
    "    \n",
    "    float_cols = data.select_dtypes(np.float64)\n",
    "    float_cols.fillna(0, inplace=True)\n",
    "    float_cols = float_cols.apply(np.int64)\n",
    "    \n",
    "    num_cols = data.select_dtypes(np.int64)\n",
    "    num_cols.drop('id', axis=1, inplace=True)\n",
    "    num_cols.fillna(0, inplace=True)\n",
    "    \n",
    "    features_data = pd.concat([num_cols, float_cols], axis=1)\n",
    "    features = features_data.columns\n",
    "    \n",
    "    return features_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c9216955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    class_res = classification_report(predictions, test_labels)\n",
    "    print(class_res)\n",
    "    return class_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fd9c9",
   "metadata": {},
   "source": [
    "# Testing different Modelling Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1e28b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      7584\n",
      "           1       0.56      0.91      0.70       249\n",
      "\n",
      "    accuracy                           0.97      7833\n",
      "   macro avg       0.78      0.94      0.84      7833\n",
      "weighted avg       0.98      0.97      0.98      7833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## RandomForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "train_feats = preprocess(train_x)\n",
    "features = train_feats.columns\n",
    "scaler =  StandardScaler()\n",
    "train_feats = scaler.fit_transform(train_feats)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_feats, train_y)\n",
    "\n",
    "test_feats = preprocess(test_x)\n",
    "test_data = scaler.transform(test_feats)\n",
    "result = evaluate(clf, test_data, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "69995d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying Randomized Search CV\n",
    "n_estimators  =[int(x) for x in np.linspace(100, 500, num=5)]\n",
    "min_samples_split = [int(x) for x in np.linspace(2, 5, num=2)]\n",
    "min_samples_leaf = [1,2,3]\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "grid_search = RandomizedSearchCV(estimator = clf, param_distributions= random_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2, n_iter=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "39515b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 3],\n",
       "                                        'min_samples_split': [2, 5],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(train_feats, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "55ccf8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      7590\n",
      "           1       0.56      0.92      0.70       243\n",
      "\n",
      "    accuracy                           0.98      7833\n",
      "   macro avg       0.78      0.95      0.84      7833\n",
      "weighted avg       0.98      0.98      0.98      7833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_search.best_params_\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "test_feats = preprocess(test_x)\n",
    "test_data = scaler.transform(test_feats)\n",
    "result = evaluate(model, test_data, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "15088637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating submission file\n",
    "test_df = pd.read_csv('test_Wf7sxXF.csv')\n",
    "test_data = preprocess(test_df)\n",
    "test_data = scaler.transform(test_data)\n",
    "preds = model.predict(test_data)\n",
    "test_df['buy'] = preds\n",
    "submit_df = test_df[['id', 'buy']]\n",
    "submit_df.to_csv(\"submission_forest.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6b31f31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      7599\n",
      "           1       0.55      0.94      0.70       234\n",
      "\n",
      "    accuracy                           0.98      7833\n",
      "   macro avg       0.78      0.96      0.84      7833\n",
      "weighted avg       0.98      0.98      0.98      7833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## SVM\n",
    "train_feats = preprocess(train_x)\n",
    "features = train_feats.columns\n",
    "scaler =  StandardScaler()\n",
    "train_feats = scaler.fit_transform(train_feats)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(train_feats, train_y)\n",
    "\n",
    "test_feats = preprocess(test_x)\n",
    "test_data = scaler.transform(test_feats)\n",
    "result = evaluate(clf, test_data, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c726ba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      7588\n",
      "           1       0.57      0.93      0.70       245\n",
      "\n",
      "    accuracy                           0.98      7833\n",
      "   macro avg       0.78      0.95      0.85      7833\n",
      "weighted avg       0.98      0.98      0.98      7833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## XGBOOST\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "train_feats = preprocess(train_x)\n",
    "features = train_feats.columns\n",
    "scaler =  StandardScaler()\n",
    "train_feats = scaler.fit_transform(train_feats)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(train_feats, train_y)\n",
    "\n",
    "test_feats = preprocess(test_x)\n",
    "test_data = scaler.transform(test_feats)\n",
    "result = evaluate(clf, test_data, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae43ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_Wf7sxXF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "24a51d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction using SVM\n",
    "test_data = preprocess(test_df)\n",
    "test_data = scaler.transform(test_data)\n",
    "preds = clf.predict(test_data)\n",
    "test_df['buy'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d8f1b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = test_df[['id', 'buy']]\n",
    "submit_df.to_csv(\"submission_svm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7e1fdc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying all binary classifiction\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7969b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"QDA\"\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    MLPClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "105e133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "Training classifier: Neural Net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu/pyenv/py3.8env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      7559\n",
      "           1       0.59      0.87      0.71       274\n",
      "\n",
      "    accuracy                           0.97      7833\n",
      "   macro avg       0.80      0.92      0.85      7833\n",
      "weighted avg       0.98      0.97      0.98      7833\n",
      "\n",
      "============================\n",
      "Training classifier: AdaBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      7591\n",
      "           1       0.57      0.94      0.71       242\n",
      "\n",
      "    accuracy                           0.98      7833\n",
      "   macro avg       0.78      0.96      0.85      7833\n",
      "weighted avg       0.98      0.98      0.98      7833\n",
      "\n",
      "============================\n",
      "Training classifier: QDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu/pyenv/py3.8env/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      7214\n",
      "           1       0.53      0.34      0.41       619\n",
      "\n",
      "    accuracy                           0.92      7833\n",
      "   macro avg       0.74      0.66      0.69      7833\n",
      "weighted avg       0.91      0.92      0.92      7833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    print(\"============================\")\n",
    "    print(\"Training classifier:\", name)\n",
    "    train_feats = preprocess(train_x)\n",
    "    features = train_feats.columns\n",
    "    scaler =  StandardScaler()\n",
    "    train_feats = scaler.fit_transform(train_feats)\n",
    "    clf.fit(train_feats, train_y)\n",
    "    test_feats = preprocess(test_x)\n",
    "    test_data = scaler.transform(test_feats)\n",
    "    result = evaluate(clf, test_data, test_y)\n",
    "    \n",
    "    #submission\n",
    "    test_df = pd.read_csv('test_Wf7sxXF.csv')\n",
    "    test_data = preprocess(test_df)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    preds = clf.predict(test_data)\n",
    "    test_df['buy'] = preds\n",
    "    \n",
    "    submit_df = test_df[['id', 'buy']]\n",
    "    submit_df.to_csv(\"submission_{}.csv\".format(name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d5a81aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu/pyenv/py3.8env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      7565\n",
      "           1       0.59      0.88      0.70       268\n",
      "\n",
      "    accuracy                           0.97      7833\n",
      "   macro avg       0.79      0.93      0.85      7833\n",
      "weighted avg       0.98      0.97      0.98      7833\n",
      "\n",
      "Test SCORE:  0.9747223286097281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     30323\n",
      "           1       0.61      0.97      0.75      1005\n",
      "\n",
      "    accuracy                           0.98     31328\n",
      "   macro avg       0.80      0.97      0.87     31328\n",
      "weighted avg       0.99      0.98      0.98     31328\n",
      "\n",
      "Train SCORE:  0.9789645045965271\n"
     ]
    }
   ],
   "source": [
    "# PCA and classification\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "train_feats = preprocess(train_x)\n",
    "features = train_feats.columns\n",
    "scaler =  StandardScaler()\n",
    "train_feats = scaler.fit_transform(train_feats)\n",
    "\n",
    "pca = PCA()\n",
    "train_pca = pca.fit_transform(train_feats)\n",
    "\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(train_pca, train_y)\n",
    "\n",
    "\n",
    "test_feats = preprocess(test_x)\n",
    "test_data = scaler.transform(test_feats)\n",
    "test_pca = pca.transform(test_data)\n",
    "\n",
    "result = evaluate(clf, test_pca, test_y)\n",
    "score = clf.score(test_pca, test_y)\n",
    "print(\"Test SCORE: \", score)\n",
    "\n",
    "result = evaluate(clf, train_pca, train_y)\n",
    "score = clf.score(train_pca, train_y)\n",
    "print(\"Train SCORE: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "04efd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID search on Neural Nets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_params = {'activation': ['tanh', 'relu'],\n",
    "              'alpha': [0.1, 0.001, 0.0001],\n",
    "              'early_stopping': [True],\n",
    "              'hidden_layer_sizes': [(50,),(100,), (150,)]\n",
    "              }\n",
    "model = MLPClassifier()\n",
    "grid_search = GridSearchCV(model, grid_params, cv = 3, n_jobs = 3, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f549b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(), n_jobs=3,\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'alpha': [0.1, 0.001, 0.0001],\n",
       "                         'early_stopping': [True],\n",
       "                         'hidden_layer_sizes': [(50,), (100,), (150,)]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats = preprocess(train_x)\n",
    "features = train_feats.columns\n",
    "scaler =  StandardScaler()\n",
    "train_feats = scaler.fit_transform(train_feats)\n",
    "\n",
    "grid_search.fit(train_feats, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0a653b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4443a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      7583\n",
      "           1       0.57      0.91      0.70       250\n",
      "\n",
      "    accuracy                           0.98      7833\n",
      "   macro avg       0.78      0.94      0.84      7833\n",
      "weighted avg       0.98      0.98      0.98      7833\n",
      "\n",
      "Test SCORE:  0.9752329886378144\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     30339\n",
      "           1       0.57      0.92      0.71       989\n",
      "\n",
      "    accuracy                           0.98     31328\n",
      "   macro avg       0.78      0.95      0.85     31328\n",
      "weighted avg       0.98      0.98      0.98     31328\n",
      "\n",
      "Train SCORE:  0.9757724719101124\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "test_feats = preprocess(test_x)\n",
    "test_data = scaler.transform(test_feats)\n",
    "result = evaluate(best_model, test_data, test_y)\n",
    "score = best_model.score(test_data, test_y)\n",
    "print(\"Test SCORE: \", score)\n",
    "\n",
    "result = evaluate(best_model, train_feats, train_y)\n",
    "score = best_model.score(train_feats, train_y)\n",
    "print(\"Train SCORE: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2a062b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    " #submission\n",
    "test_df = pd.read_csv('test_Wf7sxXF.csv')\n",
    "test_data = preprocess(test_df)\n",
    "test_data = scaler.transform(test_data)\n",
    "preds = best_model.predict(test_data)\n",
    "test_df['buy'] = preds\n",
    "\n",
    "submit_df = test_df[['id', 'buy']]\n",
    "submit_df.to_csv(\"submission_{}.csv\".format('Neural_net_GCV'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "00407a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      7579\n",
      "           1       0.60      0.94      0.73       254\n",
      "\n",
      "    accuracy                           0.98      7833\n",
      "   macro avg       0.80      0.96      0.86      7833\n",
      "weighted avg       0.99      0.98      0.98      7833\n",
      "\n",
      "Test SCORE:  0.9777862887782459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     37905\n",
      "           1       0.60      0.96      0.74      1256\n",
      "\n",
      "    accuracy                           0.98     39161\n",
      "   macro avg       0.80      0.97      0.86     39161\n",
      "weighted avg       0.99      0.98      0.98     39161\n",
      "\n",
      "Train SCORE:  0.9783968744414085\n"
     ]
    }
   ],
   "source": [
    "# Train model on complete data\n",
    "train_y = df['buy']\n",
    "train_x = df.drop('buy', axis=1)\n",
    "\n",
    "train_feats = preprocess(train_x)\n",
    "features = train_feats.columns\n",
    "scaler =  StandardScaler()\n",
    "train_feats = scaler.fit_transform(train_feats)\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(train_feats, train_y)\n",
    "\n",
    "\n",
    "test_feats = preprocess(test_x)\n",
    "test_data = scaler.transform(test_feats)\n",
    "result = evaluate(clf, test_data, test_y)\n",
    "score = clf.score(test_data, test_y)\n",
    "print(\"Test SCORE: \", score)\n",
    "\n",
    "result = evaluate(clf, train_feats, train_y)\n",
    "score = clf.score(train_feats, train_y)\n",
    "print(\"Train SCORE: \", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d3da7986",
   "metadata": {},
   "outputs": [],
   "source": [
    " #submission\n",
    "test_df = pd.read_csv('test_Wf7sxXF.csv')\n",
    "test_data = preprocess(test_df)\n",
    "test_data = scaler.transform(test_data)\n",
    "preds = clf.predict(test_data)\n",
    "test_df['buy'] = preds\n",
    "\n",
    "submit_df = test_df[['id', 'buy']]\n",
    "submit_df.to_csv(\"submission_{}.csv\".format('Neural_netfinal'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c214725",
   "metadata": {},
   "source": [
    "Final submission On Neural Nets as they gave the Highest F-score on label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072956b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
